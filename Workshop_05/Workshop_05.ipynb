{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content\n",
    "\n",
    "1. Tensors and how to work with Pytorch\n",
    "2. Autograd in Pytorch\n",
    "3. Solving a simple image classification using neural networks coded with Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install torch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "### 1.1 Creating Tensors from data\n",
    "Tensors can be created from Python lists with the `torch.Tensor()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with 1-D data\n",
      "tensor([1., 2., 3.])\n",
      "\n",
      "Example with 2-D data\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Example with 3-D data\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example with 1-D data\n",
    "data = [1.0, 2.0, 3.0]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"Example with 1-D data\")\n",
    "print(tensor)\n",
    "\n",
    "# Example with 2-D data\n",
    "data = [[1., 2., 3.], [4., 5., 6]]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"\\nExample with 2-D data\")\n",
    "print(tensor)\n",
    "\n",
    "# Example with 3-D data\n",
    "data = [[[1.,2.], [3.,4.]],\n",
    "        [[5.,6.], [7.,8.]]]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"\\nExample with 3-D data\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initializing an empty Tensor\n",
    "An uninitialized matrix is declared, but does not contain definite known values before it is used. When an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Construct a 2x3 matrix, uninitialized\n",
    "x = torch.empty(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Randomly initialized Tensor\n",
    "\n",
    "1.3 Randomly initialized Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8742, 0.7261, 0.1123],\n",
      "        [0.8335, 0.8590, 0.3504]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tensor with zeros or ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of zeros\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Matrix of zeros typecasted to long\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "\n",
      "Matrix of ones typecasted to long\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of all zeros\n",
    "x = torch.zeros(2, 3)\n",
    "print(\"Matrix of zeros\")\n",
    "print(x)\n",
    "\n",
    "# Create a matrix of all zeros and explicitly set data type to be long int\n",
    "x = torch.zeros(2, 3, dtype=torch.long)\n",
    "print(\"\\nMatrix of zeros typecasted to long\")\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(2, 3, dtype=torch.long)\n",
    "print(\"\\nMatrix of ones typecasted to long\")\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Tensor based on existing Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.0541,  0.5123,  1.3178],\n",
      "        [-0.0798, -0.3636,  0.6341]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(2, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Size of a Tensor\n",
    "`torch.Size` is in fact a tuple, so it supports all tuple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with 1-D data\n",
      "tensor([1., 2., 3.])\n",
      "torch.Size([3])\n",
      "\n",
      "Example with 2-D data\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "Example with 3-D data\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example with 1-D data\n",
    "data = [1.0, 2.0, 3.0]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"Example with 1-D data\")\n",
    "print(tensor)\n",
    "print(tensor.size())\n",
    "\n",
    "# Example with 2-D data\n",
    "data = [[1., 2., 3.], [4., 5., 6]]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"\\nExample with 2-D data\")\n",
    "print(tensor)\n",
    "print(tensor.size())\n",
    "\n",
    "# Example with 3-D data\n",
    "data = [[[1.,2.], [3.,4.]],\n",
    "        [[5.,6.], [7.,8.]]]\n",
    "tensor = torch.Tensor(data)\n",
    "print(\"\\nExample with 3-D data\")\n",
    "print(tensor)\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Operations with Tensors\n",
    "Most operations are very similar to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([5., 7., 9.])\n",
      "tensor([5., 7., 9.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/9h6cyd6s3yv14f4p2mpw0g0r0000gn/T/ipykernel_3673/1344850032.py:14: UserWarning: An output with one or more elements was resized since it had shape [2, 3], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  torch.add(x, y, out=output)\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "x = torch.Tensor([ 1., 2., 3. ])\n",
    "y = torch.Tensor([ 4., 5., 6. ])\n",
    "\n",
    "# using arithmetic operation\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# using method\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# using method and providing an output tensor as argument\n",
    "output = torch.empty(2, 3)\n",
    "torch.add(x, y, out=output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "# In-place addition\n",
    "\n",
    "x = torch.Tensor([ 1., 2., 3. ])\n",
    "y = torch.Tensor([ 4., 5., 6. ])\n",
    "\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operation that mutates a tensor in-place is post-fixed with an underscore `_`. For example: `x.copy_(y)`, `x.t_()`, will change `x`.\n",
    "\n",
    "See [the PyTorch official documentation](http://pytorch.org/docs/torch.html) for a complete list of the massive number of operations available to you.  They expand beyond just mathematical operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "# Indexing\n",
    "\n",
    "x = torch.Tensor([[1., 2., 3.], [4., 5., 6]])\n",
    "print(x[:, 1]) # Gets column with index 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7161])\n",
      "0.7161391973495483\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Converting to and from NumPy\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "\n",
    "The Torch Tensor and NumPy array will **share their underlying memory locations** (if the Torch Tensor is on CPU), and **changing one will change the other**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original a: tensor([1., 1., 1., 1., 1.])\n",
      "Original b: [1. 1. 1. 1. 1.]\n",
      "New a: tensor([2., 2., 2., 2., 2.])\n",
      "New b: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(\"Original a:\", a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(\"Original b:\", b)\n",
    "\n",
    "a.add_(1)\n",
    "print(\"New a:\", a)\n",
    "print(\"New b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. CUDA Tensors\n",
    "\n",
    "Tensors can be moved onto any device using the `.to` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "# Try to run this cell with both GPU support and without\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    x = torch.Tensor([1.0, 2.0, 3.0])\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd: Automatic Differentiation\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "``torch.Tensor`` is the central class of the package. If you set its attribute\n",
    "``.requires_grad`` as ``True``, it **starts to track all operations on it**. When\n",
    "you finish your computation you can call ``.backward()`` and have **all the\n",
    "gradients computed automatically**. The gradient for this tensor will be\n",
    "accumulated into ``.grad`` attribute.\n",
    "\n",
    "To **stop a tensor from tracking history**, you can call ``.detach()`` to detach\n",
    "it from the computation history, and to prevent future computation from being\n",
    "tracked.\n",
    "\n",
    "To **prevent tracking history (and using memory)**, you can also wrap the code block\n",
    "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
    "model because the model may have trainable parameters with `requires_grad=True`,\n",
    "but for which we don't need the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "flag in-place. The input flag defaults to ``False`` if not given.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7ff5b8012880>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK1: Find the gradiants of output ('out') with respect to x using Pytorch.\n",
    "\n",
    "Let $out = \\frac{1}{4}\\sum_i z_i$,  \n",
    "$z_i = 3(x_i+2)^2$  \n",
    "and $z_i\\bigr\\rvert_{x_i=1} = 27$.  \n",
    "Therefore,  \n",
    "$\\frac{\\partial out}{\\partial x_i} = \\frac{1}{4}\\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4}.3.2(x_i+2) = \\frac{3}{2}(x_i+2)$,  \n",
    " hence  \n",
    "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "\n",
    "Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$, compute the product $v^{T}\\cdot J$. If $v$ happens to be the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$, that is, $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$, then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n",
    "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "\n",
    "(Note that $v^{T}\\cdot J$ gives a row vector which can be treated as a column vector by taking $J^{T}\\cdot v$.)\n",
    "\n",
    "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors with `.requires_grad=True` either by wrapping the code block in with `torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Problem to solve: Classify hand written digits using the MNIST dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a set of 60,000 training\n",
    "images, plus 10,000 test images, assembled by the National Institute of Standards and\n",
    "Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the\n",
    "“Hello World” of deep learning—it’s what you do to verify that your algorithms are\n",
    "working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/img1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Hyperparameters and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and Std of MNIST dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load and Prepare the Dataset\n",
    "\n",
    "The MNIST dataset is loaded for both training and testing. We use DataLoader to manage batching and shuffling of data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 9912422/9912422 [00:04<00:00, 2044402.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 28881/28881 [00:00<00:00, 198212.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:01<00:00, 1317243.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 1149353.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the Neural Network Model\n",
    "\n",
    "We define a simple feedforward neural network with two fully connected layers. The first layer takes the flattened image (28x28 pixels) and outputs 512 features. The second layer outputs 10 classes (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the image\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Define the Loss Function, Optimizer, and Model Instance\n",
    "\n",
    "We use CrossEntropyLoss as the loss function for multi-class classification. The optimizer used is Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK2: Train the model using the given data set\n",
    "\n",
    "The training loop processes the batches, computes the gradients, and updates the model parameters. The test loop evaluates the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UBS",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
